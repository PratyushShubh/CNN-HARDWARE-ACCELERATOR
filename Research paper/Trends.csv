"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Editors","Publisher","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Document Type","Publication Stage","Open Access","Source","EID"
"Z., Lin, Zhuosheng; Z., Chen, Zaofeng; J., Zhang, Jilong; J., Deng, Jingliang; X., Wu, Xiaona; Y., Feng, Yue","Lin, Zhuosheng (56949448800); Chen, Zaofeng (59744035600); Zhang, Jilong (59744035700); Deng, Jingliang (58918085100); Wu, Xiaona (58917231000); Feng, Yue (57222404591)","56949448800; 59744035600; 59744035700; 58918085100; 58917231000; 57222404591","Weight nonlinear mapping based memristor crossbar array implementation scheme for convolutional neural network off-chip training","2026","Expert Systems with Applications","297","","129277","","","0","0","10.1016/j.eswa.2025.129277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013113162&doi=10.1016%2Fj.eswa.2025.129277&partnerID=40&md5=c57e8106da62798323ded814db50fc9f","Wuyi University, Jiangmen, China","Lin, Zhuosheng, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Chen, Zaofeng, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Zhang, Jilong, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Deng, Jingliang, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Wu, Xiaona, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Feng, Yue, School of Electronics and Information Engineering, Wuyi University, Jiangmen, China","The widespread application of memory-intensive and compute-intensive technologies such as artificial intelligence has made the “Von Neumann bottleneck” increasingly prominent. In-Memory Computing (IMC) is considered as an effective way to solve the problem of limited data throughput. In which, the memristor with intrinsic property of adjustable conductance is an ideal candidate to realize IMC. Currently, nonlinear mapping strategies have been applied to the on-chip training of neural networks. However, off-chip training is still dominated by linear mapping strategies. In this paper, we propose a novel memristor-based nonlinear mapping strategy, leveraging its intrinsic nonlinear and asymmetric conductance characteristics. Compared with the linear mapping strategy, the proposed strategy can represent more conductive states and is more consistent with the real situation of the device. On this basis, two nonlinear mapping schemes for neural network weight parameters are proposed by combining the difference and the virtual column, respectively. In order to reduce the circuit overhead, the layer structure of “convolutional layer + batch normalization layer”, which is commonly used in convolutional neural networks (CNNs), is fused. Through experiments, the influences of inherent characteristics of memristor, such as nonlinearity, the number of conductive states and ON/OFF ratio, on the performances of VGG8 model based on the two proposed schemes are investigated. The two proposed schemes are applied to five classical CNNs, including VGG, DenseNet and ResNets. Based on actual memristor parameters, the inference performances of this study are better than those of the existing schemes based on linear or hybrid mapping strategies. The two proposed schemes are applied to five classical CNNs, including VGG, DenseNet and ResNets. Based on actual memristor parameters, the inference performances of this study are better than that of the existing schemes based on linear or hybrid mapping strategies. In addition, the IMC hardware accelerator architecture is designed based on NeuroSim V1.4, and ResNet56 model is subsequently implemented. © 2025 Elsevier B.V., All rights reserved.","Cnn; Imc; Memristor; Off-chip Training; Weight Nonlinear Mapping; Brain; Computer Architecture; Conformal Mapping; Convolution; Convolutional Neural Networks; Multilayer Neural Networks; Network Layers; Convolutional Neural Network; In-memory Computing; Linear Mapping; Mapping Strategy; Memristor; Nonlinear Mappings; Off-chip; Off-chip Training; Performance; Weight Nonlinear Mapping; Memristors","Brain; Computer architecture; Conformal mapping; Convolution; Convolutional neural networks; Multilayer neural networks; Network layers; Convolutional neural network; In-memory computing; Linear mapping; Mapping strategy; Memristor; Nonlinear mappings; Off-chip; Off-chip training; Performance; Weight nonlinear mapping; Memristors","","Elsevier Ltd","09574174","","ESAPE","","English","Article","Final","","Scopus","2-s2.0-105013113162"
"B., Liu, Bingqiang; Z., Yin, Zehua; Z., Duan, Ziang; J., Xiao, Jian; Y., Tan, Yulong; J., Wang, Jipeng; Z., Shen, Zixuan; Z., Wu, Zhigang; C., Wang, Chao","Liu, Bingqiang (57267161600); Yin, Zehua (57880869600); Duan, Ziang (58834452400); Xiao, Jian (60077693000); Tan, Yulong (60025249600); Wang, Jipeng (57222009129); Shen, Zixuan (58000816000); Wu, Zhigang (60077693100); Wang, Chao (56028854400)","57267161600; 57880869600; 58834452400; 60077693000; 60025249600; 57222009129; 58000816000; 60077693100; 56028854400","A fully pipelined, low-overhead, and energy-efficient CNN-based feature extraction accelerator for mobile visual SLAM","2025","Microelectronics Journal","165","","106860","","","0","0","10.1016/j.mejo.2025.106860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014499572&doi=10.1016%2Fj.mejo.2025.106860&partnerID=40&md5=b561b217114a9ce5ce5d08332dc87f4f","Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China","Liu, Bingqiang, Huazhong University of Science and Technology, Wuhan, China; Yin, Zehua, Huazhong University of Science and Technology, Wuhan, China; Duan, Ziang, Huazhong University of Science and Technology, Wuhan, China; Xiao, Jian, School of Future Technology, Huazhong University of Science and Technology, Wuhan, China; Tan, Yulong, Huazhong University of Science and Technology, Wuhan, China; Wang, Jipeng, Huazhong University of Science and Technology, Wuhan, China; Shen, Zixuan, Huazhong University of Science and Technology, Wuhan, China; Wu, Zhigang, Huazhong University of Science and Technology, Wuhan, China; Wang, Chao, School of Future Technology, Huazhong University of Science and Technology, Wuhan, China, Huazhong University of Science and Technology, Wuhan, China","Feature extraction is critical for Visual Simultaneous Localization And Mapping (VSLAM). The CNN-based SuperPoint outperforms traditional feature extractors but its high complexity hinders deployment on energy-constrained edge devices like small mobile robots. This paper proposes an energy-efficient SuperPoint hardware accelerator for VSLAM. The key contributions are: (1) developing a lightweight SuperPoint network by reducing filter numbers based on hierarchical feature characteristics, achieving an 88.3 % reduction in model size; (2) implementing a fully pipelined architecture to avoid general-purpose processing and deep learning IP, improving energy efficiency by eliminating off-chip access and sequential computation; (3) introducing a selective descriptor convolution strategy to skip redundant calculations on non-feature points, reducing descriptor computation and hardware overhead; and (4) proposing an optimized Non-Maximum Suppression strategy to remove duplicate comparisons within the sliding windows, further enhancing energy efficiency. FPGA evaluation shows 9.09 × lower hardware overhead and 58.2 mJ/frame energy efficiency, 22.2 % better than state-of-the-art, processing 480 × 640 images at 20 fps under 200 MHz. © 2025 Elsevier B.V., All rights reserved.","Energy Efficiency; Feature Extraction; Hardware Accelerator; Slam; Superpoint; Acceleration; Computational Efficiency; Computer Hardware; Cost Reduction; Deep Learning; Extraction; Mobile Robots; Pipeline Processing Systems; Descriptors; Energy; Energy Efficient; Features Extraction; Fully Pipelined; Hardware Accelerators; Hardware Overheads; Slam; Superpoint; Visual Simultaneous Localization And Mappings; Energy Efficiency; Feature Extraction","Acceleration; Computational efficiency; Computer hardware; Cost reduction; Deep learning; Extraction; Mobile robots; Pipeline processing systems; Descriptors; Energy; Energy efficient; Features extraction; Fully pipelined; Hardware accelerators; Hardware overheads; SLAM; Superpoint; Visual simultaneous localization and mappings; Energy efficiency; Feature extraction","","Elsevier Ltd","09598324; 18792391","","MICEB","","English","Article","Final","","Scopus","2-s2.0-105014499572"
"L., Medina, Laura; J., Flich, José","Medina, Laura (57442451800); Flich, José (13611270100)","57442451800; 13611270100","SIRENA: SparsIty-REpetition aware Nibble-based hardware Accelerator for convolutional neural networks","2025","Journal of Systems Architecture","168","","103529","","","0","0","10.1016/j.sysarc.2025.103529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013544347&doi=10.1016%2Fj.sysarc.2025.103529&partnerID=40&md5=67dc716bd4edaaad7939f33ca94f45c9","Universitat Politècnica de València, Valencia, Spain","Medina, Laura, Universitat Politècnica de València, Valencia, Spain; Flich, José, Universitat Politècnica de València, Valencia, Spain","The growing demand for artificial intelligence (AI) applications demands specialized hardware accelerators to handle intensive computational loads. To reduce computing needs, this paper introduces nibble decomposition (NBD), a method that splits 8-bit values into two 4-bit nibbles to detect and remove redundant computations in convolutional neural networks (CNNs). Experiments with INT8 quantized ResNet-50, MobileNet, and YOLO-V3 show that nibble decomposition can avoid up to 91% of multiplications in the upper nibble and 70% in the lower nibble. We further propose SIRENA, an NBD hardware accelerator to optimize 8-bit quantized CNNs by skipping redundant operations without accuracy loss. Building on this method, we present SIRENA, an NBD-based accelerator that skips redundant operations without accuracy loss. Compared to a conventional value-agnostic accelerator, SIRENA achieves a 55% reduction in power consumption. © 2025 Elsevier B.V., All rights reserved.","Ai; Cnn; Hardware Accelerator; Nibble Decomposition; Acceleration; Computer Hardware; Convolutional Neural Networks; Green Computing; Signal Processing; % Reductions; Accuracy Loss; Computational Loads; Convolutional Neural Network; Growing Demand; Hardware Accelerators; Nibble Decomposition; Power; Redundant Computation; Specialized Hardware; Convolution","Acceleration; Computer hardware; Convolutional neural networks; Green computing; Signal processing; % reductions; Accuracy loss; Computational loads; Convolutional neural network; Growing demand; Hardware accelerators; Nibble decomposition; Power; Redundant computation; Specialized hardware; Convolution","","Elsevier B.V.","13837621","","JSARF","","English","Article","Final","","Scopus","2-s2.0-105013544347"
"X., Hu, Xianghong; S., Fu, Shansen; Y., Lin, Yuanmiao; X., Li, Xueming; C., Yang, Chaoming; R., Li, Rongfeng; H., Huang, Hongmin; S., Cai, Shuting; X., Xiong, Xiaoming","Hu, Xianghong (57203042497); Fu, Shansen (59656981100); Lin, Yuanmiao (59656141400); Li, Xueming (57226152987); Yang, Chaoming (59657117800); Li, Rongfeng (59656705300); Huang, Hongmin (57219941449); Cai, Shuting (35188541700); Xiong, Xiaoming (57195563270)","57203042497; 59656981100; 59656141400; 57226152987; 59657117800; 59656705300; 57219941449; 35188541700; 57195563270","An FPGA-based bit-level weight sparsity and mixed-bit accelerator for neural networks","2025","Journal of Systems Architecture","166","","103463","","","0","1","10.1016/j.sysarc.2025.103463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006878874&doi=10.1016%2Fj.sysarc.2025.103463&partnerID=40&md5=b69a42cf996083e9875d346f9431b0a5","Guangdong University of Technology, Guangzhou, China; Guangdong Polytechnic Normal University, Guangzhou, China","Hu, Xianghong, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Fu, Shansen, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Lin, Yuanmiao, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Li, Xueming, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Yang, Chaoming, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Li, Rongfeng, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Huang, Hongmin, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China, School of Electronics and Information, Guangdong Polytechnic Normal University, Guangzhou, China; Cai, Shuting, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China; Xiong, Xiaoming, School of Integrated Circuits and School of Materials and Energy, Guangdong University of Technology, Guangzhou, China","Bit-level weight sparsity and mixed-bit quantization are regarded as effective methods to improve the computing efficiency of convolutional neural network (CNN) accelerators. However, irregular sparse matrices will greatly increase the index overhead and hardware resource consumption. Moreover, bit-serial computing (BSC) is usually adopted to implement bit-level weight sparsity on accelerators, and the traditional BSC leads to uneven utilization of DSP and LUT resources on the FPGA platform, thereby limiting the improvement of the overall performance of the accelerator. Therefore, in this work, we present an accelerator designed for bit-level weight sparsity and mixed-bit quantization. We first introduce a non-linear quantization algorithm named bit-level sparsity learned quantizer (BSLQ), which can maintain high accuracy during mixed quantization and guide the accelerator to complete bit-level weight sparse computations using DSP. Based on this algorithm, we implement the multi-channel bit-level sparsity (MCBS) method to mitigate irregularities and reduce the index count associated with bit-level sparsity. Finally, we propose a sparse weight arbitrary basis scratch pad (SWAB SPad) method that enables retrieval of compressed weights without fetching activations, which can save 30.52% of LUTs and 64.02% of FFs. Experimental results demonstrate that when quantizing ResNet50 and VGG16 using 4/8 bits, our approach achieves accuracy that is comparable to or even better than 32-bit (75.98% and 73.70% for the two models). Compared to the state-of-the-art FPGA-based accelerators, this accelerator achieves up to 5.36 times DSP efficiency improvement and provides 8.87 times energy efficiency improvement. © 2025 Elsevier B.V., All rights reserved.","Bit-level Weight Sparsity; Convolutional Neural Networks; Hardware Accelerator; Mixed-bits Quantization; Convolution; Digital Arithmetic; Digital Differential Analyzers; Bit Level; Bit-level Weight Sparsity; Bit-serial; Computing Efficiency; Convolutional Neural Network; Hardware Accelerators; Mixed-bit Quantization; Neural-networks; Quantisation; Serial Computing; Convolutional Neural Networks","Convolution; Digital arithmetic; Digital differential analyzers; Bit level; Bit-level weight sparsity; Bit-serial; Computing efficiency; Convolutional neural network; Hardware accelerators; Mixed-bit quantization; Neural-networks; Quantisation; Serial computing; Convolutional neural networks","","Elsevier B.V.","13837621","","JSARF","","English","Review","Final","","Scopus","2-s2.0-105006878874"
"J., Boudjadar, Jalil; S.u., Islam, Saif ul; R., Buyya, Rajkumar","Boudjadar, Jalil (57193098972); Islam, Saif ul (57209548941); Buyya, Rajkumar (57194845546)","57193098972; 57209548941; 57194845546","Dynamic FPGA reconfiguration for scalable embedded artificial intelligence (AI): A co-design methodology for convolutional neural networks (CNN) acceleration","2025","Future Generation Computer Systems","169","","107777","","","0","2","10.1016/j.future.2025.107777","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000745253&doi=10.1016%2Fj.future.2025.107777&partnerID=40&md5=bd72cc70ee904f8d79b40bf0cc34bc99","Aarhus Universitet, Aarhus, Denmark; University of Warwick, Coventry, United Kingdom; School of Computing and Information Systems, Melbourne, Australia","Boudjadar, Jalil, Department of Electrical and Computer Engineering - Software Engineering & Computing systems, Aarhus Universitet, Aarhus, Denmark; Islam, Saif ul, University of Warwick, Coventry, United Kingdom; Buyya, Rajkumar, Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, Melbourne, Australia","In recent years, FPGA platforms have shown significant potential for accelerating artificial intelligence (AI) applications, particularly in Embedded AI. While various studies have explored adaptive AI deployment on FPGAs, there remains a gap in methodologies fully integrating software adaptability with FPGA hardware reconfigurability. This article presents a novel end-to-end co-design methodology for deploying adaptable and scalable Convolutional Neural Networks (CNNs) on FPGA platforms. The framework enhances computational performance and reduces latency by dynamically modifying hardware acceleration units by combining CNN architecture adaptability with dynamic partial reconfiguration of FPGA hardware. The proposed methodology enables automated synthesis and runtime customization of both hardware accelerators and CNN architectures, eliminating the need for iterative synthesis. This approach has been implemented and tested on a Xilinx XC7020 FPGA board for a CNN-based image classifier, achieving superior computation performance (0.68s/image) and accuracy (97%) compared to state-of-the-art alternatives. © 2025 Elsevier B.V., All rights reserved.","Adaptive Cnns; Co-design Framework; Computation Performance; Embedded Ai; Fpga Dynamic Reconfiguration; Hardware Acceleration; Scalable Ai Deployment; Integrated Circuit Design; Printed Circuit Design; Reconfigurable Architectures; Reconfigurable Hardware; Adaptive Convolutional Neural Network; Co-design Framework; Co-designs; Computation Performance; Convolutional Neural Network; Design Frameworks; Embedded Artificial Intelligence; Fpga Dynamic Reconfiguration; Hardware Acceleration; Scalable Artificial Intelligence Deployment; Convolutional Neural Networks","Integrated circuit design; Printed circuit design; Reconfigurable architectures; Reconfigurable hardware; Adaptive convolutional neural network; Co-design framework; Co-designs; Computation performance; Convolutional neural network; Design frameworks; Embedded artificial intelligence; FPGA dynamic reconfiguration; Hardware acceleration; Scalable artificial intelligence deployment; Convolutional neural networks","","Elsevier B.V.","0167739X","","FGCSE","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-86000745253"
"A., Gundrapally, Achyuth; Y.A., Shah, Yatrik Ashish; S.M., Vemuri, Sai Manohar; K., Choi, Kyuwon","Gundrapally, Achyuth (59216437200); Shah, Yatrik Ashish (59215921900); Vemuri, Sai Manohar (60075573100); Choi, Kyuwon (23975486700)","59216437200; 59215921900; 60075573100; 23975486700","Hardware Accelerator Design by Using RT-Level Power Optimization Techniques on FPGA for Future AI Mobile Applications","2025","Electronics (Switzerland)","14","16","3317","","","0","0","10.3390/electronics14163317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014329980&doi=10.3390%2Felectronics14163317&partnerID=40&md5=320a25f1d303f4e279f1ad0cfed7262a","Armour College of Engineering, Chicago, United States","Gundrapally, Achyuth, DA-Lab, Armour College of Engineering, Chicago, United States; Shah, Yatrik Ashish, DA-Lab, Armour College of Engineering, Chicago, United States; Vemuri, Sai Manohar, DA-Lab, Armour College of Engineering, Chicago, United States; Choi, Kyuwon, DA-Lab, Armour College of Engineering, Chicago, United States","In resource-constrained edge environments—such as mobile devices, IoT systems, and electric vehicles—energy-efficient Convolution Neural Network (CNN) accelerators on mobile Field Programmable Gate Arrays (FPGAs) are gaining significant attention for real-time object detection tasks. This paper presents a low-power implementation of the Tiny YOLOv4 object detection model on the Xilinx ZCU104 FPGA platform by using Register Transfer Level (RTL) optimization techniques. We proposed three RTL techniques in the paper: (i) Local Explicit Clock Enable (LECE), (ii) operand isolation, and (iii) Enhanced Clock Gating (ECG). A novel low-power design of Multiply-Accumulate (MAC) operations, which is one of the main components in the AI algorithm, was proposed to eliminate redundant signal switching activities. The Tiny YOLOv4 model, trained on the COCO dataset, was quantized and compiled using the Tensil tool-chain for fixed-point inference deployment. Post-implementation evaluation using Vivado 2022.2 demonstrates around 29.4% reduction in total on-chip power. Our design supports real-time detection throughput while maintaining high accuracy, making it ideal for deployment in battery-constrained environments such as drones, surveillance systems, and autonomous vehicles. These results highlight the effectiveness of RTL-level power optimization for scalable and sustainable edge AI deployment. © 2025 Elsevier B.V., All rights reserved.","Bram; Cnn Accelerator; Cnn Architecture; Fpga Rt Level Design; High Performance; Low-power Techniques; Mac; Object Detection; Operand Isolation; Power Consumption; Acceleration; Computer Aided Design; Constrained Optimization; Electric Clocks; Electric Power Supplies To Apparatus; Electric Power Utilization; Energy Efficiency; Field Programmable Gate Arrays (fpga); Integrated Circuit Design; Low Power Electronics; Object Recognition; Real Time Systems; Bram; Convolution Neural Network; Convolution Neural Network Accelerator; Convolution Neural Network Architecture; Field Programmable Gate Array Rt Level Design; Field Programmables; High Performance; Level Design; Low Power Techniques; Multiplyaccumulate (mac); Neural Network Architecture; Objects Detection; Operand Isolation; Performance; Power; Programmable Gate Array; Object Detection","Acceleration; Computer aided design; Constrained optimization; Electric clocks; Electric power supplies to apparatus; Electric power utilization; Energy efficiency; Field programmable gate arrays (FPGA); Integrated circuit design; Low power electronics; Object recognition; Real time systems; BRAM; Convolution neural network; Convolution neural network accelerator; Convolution neural network architecture; Field programmable gate array RT level design; Field programmables; High performance; Level design; Low power techniques; Multiplyaccumulate (MAC); Neural network architecture; Objects detection; Operand isolation; Performance; Power; Programmable gate array; Object detection","","Multidisciplinary Digital Publishing Institute (MDPI)","20799292","","","","English","Article","Final","","Scopus","2-s2.0-105014329980"
"Y., Liang, Yupei; W., Chao, Wenchin; C.C., Chung, Ching Che","Liang, Yupei (57193836864); Chao, Wenchin (60041332400); Chung, Ching Che (57762571600)","57193836864; 60041332400; 57762571600","Low-Power Branch CNN Hardware Accelerator with Early Exit for UAV Disaster Detection Using 16 nm CMOS Technology","2025","Sensors","25","15","4867","","","0","0","10.3390/s25154867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013200444&doi=10.3390%2Fs25154867&partnerID=40&md5=506b88617b89eb556ea7f2071d0a85cc","National Chung Cheng University, Min-Hsiung, Taiwan","Liang, Yupei, Advanced Institute of Manufacturing with High-Tech Innovations, National Chung Cheng University, Min-Hsiung, Taiwan; Chao, Wenchin, Advanced Institute of Manufacturing with High-Tech Innovations, National Chung Cheng University, Min-Hsiung, Taiwan; Chung, Ching Che, Advanced Institute of Manufacturing with High-Tech Innovations, National Chung Cheng University, Min-Hsiung, Taiwan","This paper presents a disaster detection framework based on aerial imagery, utilizing a Branch Convolutional Neural Network (B-CNN) to enhance feature learning efficiency. The B-CNN architecture incorporates branch training, enabling effective training and inference with reduced model parameters. To further optimize resource usage, the framework integrates DoReFa-Net for weight quantization and fixed-point parameter representation. An early exit mechanism is introduced to support low-latency, energy-efficient predictions. The proposed B-CNN hardware accelerator is implemented using TSMC 16 nm CMOS technology, incorporating power gating techniques to manage memory power consumption. Post-layout simulations demonstrate that the proposed hardware accelerator operates at 500 MHz with a power consumption of 37.56 mW. The system achieves a disaster prediction accuracy of 88.18%, highlighting its effectiveness and suitability for low-power, real-time applications in aerial disaster monitoring. © 2025 Elsevier B.V., All rights reserved.","Digital Circuits; Disaster Detection; Early-exit Mechanism; Fixed-point Arithmetic; Neural Networks; Quantization; Real-time Systems; Unmanned Aerial Vehicles (uavs); Acceleration; Aerial Photography; Aircraft Detection; Antennas; Cmos Integrated Circuits; Computer Hardware; Convolutional Neural Networks; Disasters; Electric Power Utilization; Energy Efficiency; Fixed Point Arithmetic; Interactive Computer Systems; Low Power Electronics; Quantization (signal); Unmanned Aerial Vehicles (uav); Aerial Vehicle; Disaster Detection; Early-exit Mechanism; Exit Mechanisms; Fixed-point Arithmetics; Hardware Accelerators; Neural-networks; Quantisation; Real - Time System; Unmanned Aerial Vehicle; Real Time Systems; Arithmetic; Article; Convolutional Neural Network; Digital Circuit; Disaster; Human; Imagery; Latent Period; Major Clinical Study; Nerve Cell Network; Prediction; Quantization; Simulation; Unmanned Aerial Vehicle","Acceleration; Aerial photography; Aircraft detection; Antennas; CMOS integrated circuits; Computer hardware; Convolutional neural networks; Disasters; Electric power utilization; Energy efficiency; Fixed point arithmetic; Interactive computer systems; Low power electronics; Quantization (signal); Unmanned aerial vehicles (UAV); Aerial vehicle; Disaster detection; Early-exit mechanism; Exit mechanisms; Fixed-point arithmetics; Hardware accelerators; Neural-networks; Quantisation; Real - Time system; Unmanned aerial vehicle; Real time systems; arithmetic; article; convolutional neural network; digital circuit; disaster; human; imagery; latent period; major clinical study; nerve cell network; prediction; quantization; simulation; unmanned aerial vehicle","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","40808031","English","Article","Final","","Scopus","2-s2.0-105013200444"
"H., K, Hemalatha; R., Sakthivel, R.","K, Hemalatha (57209199550); Sakthivel, R. (55247470400)","57209199550; 55247470400","Review on FPGA-based hardware accelerators of CNN for healthcare applications","2025","","","","","64","87","0","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014041235&partnerID=40&md5=1e42824e2b5b1b72138068b6c729a194","Vellore Institute of Technology, Vellore, India","K, Hemalatha, Vellore Institute of Technology, Vellore, India; Sakthivel, R., Vellore Institute of Technology, Vellore, India","In recent years, Deep Convolutional Neural Network (CNN) has been the fastest-growing area of Artificial Neural Network (ANN). In addition to image classification and segmentation, CNN can detect objects in video and recognize speech. This is because CNNs take a lot of computation. The CNN function also lends itself to programmable hardware such as Field Programmable Gate Arrays (FPGAs). Recently, hardware accelerators have become incredibly popular for a broad spectrum of healthcare applications. The emergence of edge computing has made it possible to combine a large number of sensors and process information using lightweight computing. Deep learning algorithms have advanced significantly over time, providing intriguing prospects for their use even in safety-sensitive biomedical and healthcare applications. This study presents a thorough analysis and discussion of several difficulties in the implementation of FPGA-based hardware acceleration for healthcare applications. There are some clear advantages that a variety of generalized new architectures and devices have over traditional processing units. This survey is expected to be useful for researchers in the area of artificial intelligence, FPGA-based hardware accelerators of CNN for Biomedical applications, and system design. © 2025 Elsevier B.V., All rights reserved.","Convolutional Neural Networks; Fpga; Hardware Accelerator; Power Efficiency; Reliability; Security; Acceleration; Computer Architecture; Computer Hardware; Convolution; Convolutional Neural Networks; Deep Neural Networks; Health Care; Integrated Circuit Design; Learning Algorithms; Medical Applications; Network Security; Systems Analysis; Biomedical Applications; Convolutional Neural Network; Field Programmable Gate Array; Field Programmables; Hardware Accelerators; Health Care Application; Neural-networks; Power-efficiency; Programmable Gate Array; Security; Field Programmable Gate Arrays (fpga); Reliability","Acceleration; Computer architecture; Computer hardware; Convolution; Convolutional neural networks; Deep neural networks; Health care; Integrated circuit design; Learning algorithms; Medical applications; Network security; Systems analysis; Biomedical applications; Convolutional neural network; Field programmable gate array; Field programmables; Hardware accelerators; Health care application; Neural-networks; Power-efficiency; Programmable gate array; Security; Field programmable gate arrays (FPGA); Reliability","","Bentham Science Publishers","","9789815274141; 9789815274134","","","English","Book chapter","Final","","Scopus","2-s2.0-105014041235"
"G.B., Thieu, Gia Bao; S., Gesper, Sven; G., Payá-Vayá, Guillermo","Thieu, Gia Bao (58237633800); Gesper, Sven (57210786910); Payá-Vayá, Guillermo (19640447600)","58237633800; 57210786910; 19640447600","DCMA: Accelerating Parallel DMA Transfers with a Multi-Port Direct Cached Memory Access in a Massive-Parallel Vector Processor","2025","ACM Transactions on Architecture and Code Optimization","22","2","72","","","0","0","10.1145/3730582","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010732352&doi=10.1145%2F3730582&partnerID=40&md5=a17431349320cf902dd461f70ed6c8e1","Technische Universität Braunschweig, Braunschweig, Germany","Thieu, Gia Bao, Technische Universität Braunschweig, Braunschweig, Germany; Gesper, Sven, Technische Universität Braunschweig, Braunschweig, Germany; Payá-Vayá, Guillermo, Technische Universität Braunschweig, Braunschweig, Germany","State-of-the-art applications, such as convolutional neural networks, demand specialized hardware accelerators that address performance and efficiency constraints. An efficient memory hierarchy is mandatory for such hardware systems. While the memory architectures of general-purpose processors (e.g., CPU or GPUs) are based on cache systems, dedicated accelerators have mostly adopted the DMA (Direct Memory Access) concept due to the application field of image processing. DMA features like 2D data transfers or data padding can optimize the memory accesses of image processing. However, DMA lacks the capability to exploit temporal and spatial data reuse, a feature common in cache systems, particularly when multiple DMAs operate in parallel. This article proposes a novel Direct Cached Memory Access (DCMA) architecture, combining both DMA and cache methodologies and their respective advantages. Optimized for image-based AI algorithms, the DCMA architecture facilitates enhanced memory access by integrating multiple, parallel DMA ports with caching capabilities. This design allows for efficient data reuse and parallel memory access. Optimal parameters for the DCMA are determined through a comprehensive design space exploration. The DCMA is evaluated on a state-of-the-art Xilinx UltraScale+ FPGA board coupled with a massive-parallel vertical vector co-processor, called V2PRO. The results show the mitigation of the vector processor's memory bottleneck. By using the proposed DCMA, speedups of up to ×17 for the ResNet-50 CNN can be achieved. © 2025 Elsevier B.V., All rights reserved.","Cache; Dma; Hybrid Dma-cache; Machine Learning; Memory Hierarchy; Vertical Vector Processor; Arts Computing; Cache Memory; Data Transfer; Memory Architecture; Network Architecture; Neural Networks; Parallel Architectures; Program Processors; Space Research; Vectors; Cache; Direct Memory Access; Hybrid Direct Memory Access-cache; Machine-learning; Memory Access; Memory Hierarchy; State Of The Art; Vector Processors; Vertical Vector Processor; Image Enhancement","Arts computing; Cache memory; Data transfer; Memory architecture; Network architecture; Neural networks; Parallel architectures; Program processors; Space research; Vectors; Cache; Direct memory access; Hybrid direct memory access-cache; Machine-learning; Memory access; Memory hierarchy; State of the art; Vector processors; Vertical vector processor; Image enhancement","","Association for Computing Machinery","15443973; 15443566","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105010732352"
"M.A., Akbar, Muhammad Ali; B., Wang, Bo; S.B., Belhaouari, Samir Brahim; A., Bermak, Amine","Akbar, Muhammad Ali (56816492500); Wang, Bo (57192090595); Belhaouari, Samir Brahim (35109905700); Bermak, Amine (6701561781)","56816492500; 57192090595; 35109905700; 6701561781","DROPc-Dynamic Resource Optimization for Convolution Layer","2025","Electronics (Switzerland)","14","13","2658","","","0","0","10.3390/electronics14132658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010339417&doi=10.3390%2Felectronics14132658&partnerID=40&md5=523f0e0395ad716d3fa114a58e19a868","Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar","Akbar, Muhammad Ali, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Wang, Bo, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Belhaouari, Samir Brahim, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Bermak, Amine, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar","The computational complexity of convolutional neural networks (CNNs) becomes challenging for resource-constrained hardware devices. The convolution layer is predominant in the overall CNN architecture, performing the expensive multiplication and accumulation operation. Therefore, designing a hardware-efficient convolution layer will effectively improve the overall performance of a CNN. In this research, we propose a dynamic resource optimization (DROP) approach to improve the power and delay of the convolution layer. The proposed approach controls the computational path in accordance to the interrupts which are dependent on a non-zero-bit pattern. With a single interrupt, our solution provides 42.5% power and 36.7% delay efficiency compared to the standard bit-serial-parallel approach. Moreover, the power consumed by eight parallel functioning blocks is 27.7% less than the traditional bit-parallel approach. © 2025 Elsevier B.V., All rights reserved.","Convolution; Convolutional Neural Network; Hardware Accelerator; Computer Hardware; Convolutional Neural Networks; Drops; Resource Allocation; Approach Controls; Convolutional Neural Network; Dynamic Resources; Hardware Accelerators; Hardware Devices; Neural Network Architecture; Optimization Approach; Performance; Power; Resources Optimization; Convolution","Computer hardware; Convolutional neural networks; Drops; Resource allocation; Approach controls; Convolutional neural network; Dynamic resources; Hardware accelerators; Hardware devices; Neural network architecture; Optimization approach; Performance; Power; Resources optimization; Convolution","","Multidisciplinary Digital Publishing Institute (MDPI)","20799292","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105010339417"
